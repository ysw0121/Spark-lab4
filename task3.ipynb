{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, lit,to_date,date_diff\n",
    "from pyspark.ml.regression import IsotonicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------------+\n",
      "|report_date|total_purchase_amt|total_redeem_amt|\n",
      "+-----------+------------------+----------------+\n",
      "|   20140824|         130195484|       191080151|\n",
      "|   20140812|         258493673|       309754858|\n",
      "|   20140728|         371762756|       345986909|\n",
      "|   20140716|         394890140|       234775948|\n",
      "|   20140704|         211649838|       264494550|\n",
      "|   20140320|         365011495|       336076380|\n",
      "|   20140404|         251895894|       200192637|\n",
      "|   20140416|         387847838|       255914640|\n",
      "|   20140428|         324937272|       327724735|\n",
      "|   20140512|         325108597|       293952908|\n",
      "|   20140524|         160073254|       154409868|\n",
      "|   20140608|         302171269|       169525332|\n",
      "|   20140620|         251582530|       286583065|\n",
      "|   20131020|          47766681|        50884342|\n",
      "|   20131104|         300027403|       130970051|\n",
      "|   20131116|         118085705|        28996272|\n",
      "|   20131128|         139760425|        61453591|\n",
      "|   20131212|         140360007|        92090275|\n",
      "|   20140308|         243274169|       140323202|\n",
      "|   20140224|         656317045|       473470156|\n",
      "+-----------+------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"ARIMA\").getOrCreate()\n",
    "data_path=\"file:///home/siwenyu/桌面/Spark-lab4/res/task1_1/part-00000-3a410553-5280-4da6-8b49-741df2889028-c000.csv\"\n",
    "df=spark.read.csv(data_path,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------------+--------+\n",
      "|report_date|total_purchase_amt|total_redeem_amt|sequence|\n",
      "+-----------+------------------+----------------+--------+\n",
      "| 2014-08-24|         130195484|       191080151|     419|\n",
      "| 2014-08-12|         258493673|       309754858|     407|\n",
      "| 2014-07-28|         371762756|       345986909|     392|\n",
      "| 2014-07-16|         394890140|       234775948|     380|\n",
      "| 2014-07-04|         211649838|       264494550|     368|\n",
      "| 2014-03-20|         365011495|       336076380|     262|\n",
      "| 2014-04-04|         251895894|       200192637|     277|\n",
      "| 2014-04-16|         387847838|       255914640|     289|\n",
      "| 2014-04-28|         324937272|       327724735|     301|\n",
      "| 2014-05-12|         325108597|       293952908|     315|\n",
      "| 2014-05-24|         160073254|       154409868|     327|\n",
      "| 2014-06-08|         302171269|       169525332|     342|\n",
      "| 2014-06-20|         251582530|       286583065|     354|\n",
      "| 2013-10-20|          47766681|        50884342|     111|\n",
      "| 2013-11-04|         300027403|       130970051|     126|\n",
      "| 2013-11-16|         118085705|        28996272|     138|\n",
      "| 2013-11-28|         139760425|        61453591|     150|\n",
      "| 2013-12-12|         140360007|        92090275|     164|\n",
      "| 2014-03-08|         243274169|       140323202|     250|\n",
      "| 2014-02-24|         656317045|       473470156|     238|\n",
      "+-----------+------------------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# preprocess\n",
    "# date conversion to yyMMdd, but first select a base\n",
    "base_date = F.lit(\"2013-07-01\").cast(\"date\")\n",
    "df = df.withColumn(\"report_date\", F.to_date(col(\"report_date\"), \"yyyyMMdd\"))\n",
    "# sequence based on base\n",
    "df = df.withColumn(\"sequence\", F.datediff(col(\"report_date\"), base_date))\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|sequence|total_purchase_amt|\n",
      "+--------+------------------+\n",
      "|       0|          32488348|\n",
      "|       1|          29037390|\n",
      "|       2|          27270770|\n",
      "|       3|          18321185|\n",
      "|       4|          11648749|\n",
      "|       5|          36751272|\n",
      "|       6|           8962232|\n",
      "|       7|          57258266|\n",
      "|       8|          26798941|\n",
      "|       9|          30696506|\n",
      "|      10|          44075197|\n",
      "|      11|          34183904|\n",
      "|      12|          15164717|\n",
      "|      13|          22615303|\n",
      "|      14|          48128555|\n",
      "|      15|          50622847|\n",
      "|      16|          29015682|\n",
      "|      17|          24234505|\n",
      "|      18|          33680124|\n",
      "|      19|          20439079|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------------+\n",
      "|sequence|total_redeem_amt|\n",
      "+--------+----------------+\n",
      "|       0|         5525022|\n",
      "|       1|         2554548|\n",
      "|       2|         5953867|\n",
      "|       3|         6410729|\n",
      "|       4|         2763587|\n",
      "|       5|         1616635|\n",
      "|       6|         3982735|\n",
      "|       7|         8347729|\n",
      "|       8|         3473059|\n",
      "|       9|         2597169|\n",
      "|      10|         3508800|\n",
      "|      11|         8492573|\n",
      "|      12|         3482829|\n",
      "|      13|         2784107|\n",
      "|      14|        13107943|\n",
      "|      15|        11864981|\n",
      "|      16|        10911513|\n",
      "|      17|        11765356|\n",
      "|      18|         9244769|\n",
      "|      19|         4601143|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only select columns: sequence, total_purchase_amt; sequence, total_redeem_amt, 2 dataframes\n",
    "purchase = df.select(\"sequence\", \"total_purchase_amt\").orderBy(\"sequence\")\n",
    "redeem = df.select(\"sequence\", \"total_redeem_amt\").orderBy(\"sequence\")\n",
    "purchase.show()\n",
    "redeem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|sequence|total_purchase_amt|\n",
      "+--------+------------------+\n",
      "+--------+------------------+\n",
      "\n",
      "+--------+----------------+\n",
      "|sequence|total_redeem_amt|\n",
      "+--------+----------------+\n",
      "+--------+----------------+\n",
      "\n",
      "+--------+------------------+\n",
      "|sequence|total_purchase_amt|\n",
      "+--------+------------------+\n",
      "|       0|       3.2488348E7|\n",
      "|       1|        2.903739E7|\n",
      "|       2|        2.727077E7|\n",
      "|       3|       1.8321185E7|\n",
      "|       4|       1.1648749E7|\n",
      "|       5|       3.6751272E7|\n",
      "|       6|         8962232.0|\n",
      "|       7|       5.7258266E7|\n",
      "|       8|       2.6798941E7|\n",
      "|       9|       3.0696506E7|\n",
      "|      10|       4.4075197E7|\n",
      "|      11|       3.4183904E7|\n",
      "|      12|       1.5164717E7|\n",
      "|      13|       2.2615303E7|\n",
      "|      14|       4.8128555E7|\n",
      "|      15|       5.0622847E7|\n",
      "|      16|       2.9015682E7|\n",
      "|      17|       2.4234505E7|\n",
      "|      18|       3.3680124E7|\n",
      "|      19|       2.0439079E7|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------------+\n",
      "|sequence|total_redeem_amt|\n",
      "+--------+----------------+\n",
      "|       0|       5525022.0|\n",
      "|       1|       2554548.0|\n",
      "|       2|       5953867.0|\n",
      "|       3|       6410729.0|\n",
      "|       4|       2763587.0|\n",
      "|       5|       1616635.0|\n",
      "|       6|       3982735.0|\n",
      "|       7|       8347729.0|\n",
      "|       8|       3473059.0|\n",
      "|       9|       2597169.0|\n",
      "|      10|       3508800.0|\n",
      "|      11|       8492573.0|\n",
      "|      12|       3482829.0|\n",
      "|      13|       2784107.0|\n",
      "|      14|     1.3107943E7|\n",
      "|      15|     1.1864981E7|\n",
      "|      16|     1.0911513E7|\n",
      "|      17|     1.1765356E7|\n",
      "|      18|       9244769.0|\n",
      "|      19|       4601143.0|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess for isotonic regression\n",
    "\n",
    "# check the null values\n",
    "purchase.where(col(\"total_purchase_amt\").isNull()).show()\n",
    "redeem.where(col(\"total_redeem_amt\").isNull()).show()\n",
    "\n",
    "# transfer amt to double\n",
    "purchase = purchase.withColumn(\"total_purchase_amt\", col(\"total_purchase_amt\").cast(\"double\"))\n",
    "redeem = redeem.withColumn(\"total_redeem_amt\", col(\"total_redeem_amt\").cast(\"double\"))\n",
    "purchase.show()\n",
    "redeem.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n",
      "+--------+\n",
      "|sequence|\n",
      "+--------+\n",
      "|     427|\n",
      "|     428|\n",
      "|     429|\n",
      "|     430|\n",
      "|     431|\n",
      "|     432|\n",
      "|     433|\n",
      "|     434|\n",
      "|     435|\n",
      "|     436|\n",
      "|     437|\n",
      "|     438|\n",
      "|     439|\n",
      "|     440|\n",
      "|     441|\n",
      "|     442|\n",
      "|     443|\n",
      "|     444|\n",
      "|     445|\n",
      "|     446|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|sequence|\n",
      "+--------+\n",
      "|     427|\n",
      "|     428|\n",
      "|     429|\n",
      "|     430|\n",
      "|     431|\n",
      "|     432|\n",
      "|     433|\n",
      "|     434|\n",
      "|     435|\n",
      "|     436|\n",
      "|     437|\n",
      "|     438|\n",
      "|     439|\n",
      "|     440|\n",
      "|     441|\n",
      "|     442|\n",
      "|     443|\n",
      "|     444|\n",
      "|     445|\n",
      "|     446|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column features already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m future_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m purchase \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpurchase\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_purchase_amt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# add a featurecol in the dataframe with tuple\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model_pur \u001b[38;5;241m=\u001b[39m IsotonicRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_purchase_amt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column features already exists."
     ]
    }
   ],
   "source": [
    "# train isotonic regression model\n",
    "\n",
    "\n",
    "# predict\n",
    "\n",
    "from pyspark.sql.functions import date_diff, lit, to_date\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "max_date = purchase.selectExpr(\"max(sequence) as sequence\").collect()[0][\"sequence\"]\n",
    "print(max_date)\n",
    "future_day_diff =[(max_date+i,) for i in range(1,31)]\n",
    "future=spark.createDataFrame(future_day_diff,[\"sequence\"])\n",
    "future.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"sequence\"], outputCol=\"features\")\n",
    "future_df = assembler.transform(future).select(\"sequence\")\n",
    "future_df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"sequence\"], outputCol=\"features\")\n",
    "purchase = assembler.transform(purchase).select(\"features\", col(\"total_purchase_amt\").alias(\"label\"))\n",
    "\n",
    "# add a featurecol in the dataframe with tuple\n",
    "model_pur = IsotonicRegression(featuresCol=\"sequence\", labelCol=\"total_purchase_amt\")\n",
    "model_red = IsotonicRegression(featuresCol=\"sequence\", labelCol=\"total_redeem_amt\")\n",
    "\n",
    "purchase_pred=model_pur.fit(purchase).transform(future_df)\n",
    "\n",
    "purchase_pred.show()\n",
    "redeem_pred.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
